{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA, default_quantize\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    "    quantize: bool,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time()\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "\n",
    "    torch.set_default_tensor_type(torch.HalfTensor)\n",
    "    print(\"Allocating transformer on host\")\n",
    "    ctx_tok = default_quantize.set(quantize)\n",
    "    model = Transformer(model_args)\n",
    "    default_quantize.reset(ctx_tok)\n",
    "    key_to_dim = {\n",
    "        \"w1\": 0,\n",
    "        \"w2\": -1,\n",
    "        \"w3\": 0,\n",
    "        \"wo\": -1,\n",
    "        \"wq\": 0,\n",
    "        \"wk\": 0,\n",
    "        \"wv\": 0,\n",
    "        \"output\": 0,\n",
    "        \"tok_embeddings\": -1,\n",
    "        \"ffn_norm\": None,\n",
    "        \"attention_norm\": None,\n",
    "        \"norm\": None,\n",
    "        \"rope\": None,\n",
    "    }\n",
    "\n",
    "    # ?\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "    # for i, ckpt in enumerate(checkpoints):\n",
    "    #     print(f\"Loading checkpoint {i}\")\n",
    "    #     checkpoint = torch.load(ckpt, map_location=\"cpu\")\n",
    "    #     for parameter_name, parameter in model.named_parameters():\n",
    "    #         short_name = parameter_name.split(\".\")[-2]\n",
    "    #         if key_to_dim[short_name] is None and i == 0:\n",
    "    #             parameter.data = checkpoint[parameter_name]\n",
    "    #         elif key_to_dim[short_name] == 0:\n",
    "    #             size = checkpoint[parameter_name].size(0)\n",
    "    #             parameter.data[size * i : size * (i + 1), :] = checkpoint[\n",
    "    #                 parameter_name\n",
    "    #             ]\n",
    "    #         elif key_to_dim[short_name] == -1:\n",
    "    #             size = checkpoint[parameter_name].size(-1)\n",
    "    #             parameter.data[:, size * i : size * (i + 1)] = checkpoint[\n",
    "    #                 parameter_name\n",
    "    #             ]\n",
    "    #         del checkpoint[parameter_name]\n",
    "    #     del checkpoint\n",
    "    return model   \n",
    "    # model.cuda()\n",
    "\n",
    "    # generator = LLaMA(model, tokenizer)\n",
    "    # print(\n",
    "    #     f\"Loaded in {time.time() - start_time:.2f} seconds with {torch.cuda.max_memory_allocated() / 1024 ** 3:.2f} GiB\"\n",
    "    # )\n",
    "    # return generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating transformer on host\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     11\u001b[0m generator \u001b[39m=\u001b[39m load(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, use_int8)\n\u001b[1;32m     13\u001b[0m prompts \u001b[39m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[39m# For these prompts, the expected answer is the natural continuation of the prompt\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Welcome.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mDalio:\"\"\"\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m ]\n\u001b[0;32m---> 23\u001b[0m results \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     24\u001b[0m     prompts,\n\u001b[1;32m     25\u001b[0m     max_gen_len\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,\n\u001b[1;32m     26\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m     27\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m     28\u001b[0m     repetition_penalty_range\u001b[39m=\u001b[39mrepetition_penalty_range,\n\u001b[1;32m     29\u001b[0m     repetition_penalty_slope\u001b[39m=\u001b[39mrepetition_penalty_slope,\n\u001b[1;32m     30\u001b[0m     repetition_penalty\u001b[39m=\u001b[39mrepetition_penalty,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/llama/llama-int8/tmp/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "temperature = 0.8\n",
    "top_p = 0.95\n",
    "repetition_penalty_range = 1024\n",
    "repetition_penalty_slope = 0\n",
    "repetition_penalty = 1.15\n",
    "max_seq_len = 512\n",
    "max_batch_size = 32\n",
    "ckpt_dir = \"/mnt/8b4bef76-4da0-408d-bbc6-970c03cccc81/1_data_backup/llama2/github/llama-2-13b\"\n",
    "tokenizer_path = \"/home/usr1/llama/llama-main/tokenizer.model\"\n",
    "use_int8 = False\n",
    "generator = load(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, use_int8)\n",
    "\n",
    "prompts = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"\"\"Welcome.\n",
    "The following conversation took place at Harvard University.\n",
    "Former Treasurer Secretary Larry Summers invited Ray Dalio, the founder, chairman and\n",
    "co-CIO of Bridgewater Associates, the world's largest hedge fund, to discuss Dalio's unique\n",
    "views on economics.\n",
    "\n",
    "Dalio:\"\"\",\n",
    "]\n",
    "results = generator.generate(\n",
    "    prompts,\n",
    "    max_gen_len=1024,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    repetition_penalty_range=repetition_penalty_range,\n",
    "    repetition_penalty_slope=repetition_penalty_slope,\n",
    "    repetition_penalty=repetition_penalty,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating transformer on host\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.8\n",
    "top_p = 0.95\n",
    "repetition_penalty_range = 1024\n",
    "repetition_penalty_slope = 0\n",
    "repetition_penalty = 1.15\n",
    "max_seq_len = 512\n",
    "max_batch_size = 32\n",
    "ckpt_dir = \"/mnt/8b4bef76-4da0-408d-bbc6-970c03cccc81/1_data_backup/llama2/github/llama-2-13b\"\n",
    "tokenizer_path = \"/home/usr1/llama/llama-main/tokenizer.model\"\n",
    "use_int8 = True\n",
    "generator = load(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, use_int8)\n",
    "\n",
    "prompts = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"\"\"Welcome.\n",
    "The following conversation took place at Harvard University.\n",
    "Former Treasurer Secretary Larry Summers invited Ray Dalio, the founder, chairman and\n",
    "co-CIO of Bridgewater Associates, the world's largest hedge fund, to discuss Dalio's unique\n",
    "views on economics.\n",
    "\n",
    "Dalio:\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Int8Params([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.layers[0].attention.wq.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter containing:\n",
    "tensor([[ 0.0002, -0.0022, -0.0065,  ...,  0.0009,  0.0042,  0.0032],\n",
    "        [-0.0004,  0.0054,  0.0111,  ..., -0.0017,  0.0069,  0.0015],\n",
    "        [ 0.0007,  0.0016,  0.0051,  ...,  0.0027,  0.0038,  0.0018],\n",
    "        ...,\n",
    "        [ 0.0036,  0.0068,  0.0061,  ...,  0.0051, -0.0029,  0.0004],\n",
    "        [-0.0051, -0.0066, -0.0018,  ..., -0.0067,  0.0030, -0.0010],\n",
    "        [-0.0029, -0.0052, -0.0066,  ..., -0.0046,  0.0033,  0.0001]],\n",
    "       dtype=torch.float16, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter containing:\n",
    "Parameter(Int8Params([[ 0.0002, -0.0022, -0.0065,  ...,  0.0009,  0.0042,  0.0032],\n",
    "            [-0.0004,  0.0054,  0.0111,  ..., -0.0017,  0.0069,  0.0015],\n",
    "            [ 0.0007,  0.0016,  0.0051,  ...,  0.0027,  0.0038,  0.0018],\n",
    "            ...,\n",
    "            [ 0.0036,  0.0068,  0.0061,  ...,  0.0051, -0.0029,  0.0004],\n",
    "            [-0.0051, -0.0066, -0.0018,  ..., -0.0067,  0.0030, -0.0010],\n",
    "            [-0.0029, -0.0052, -0.0066,  ..., -0.0046,  0.0033,  0.0001]],\n",
    "           dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"13b_int8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
